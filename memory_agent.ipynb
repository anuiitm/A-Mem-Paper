{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef22b138",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ba45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f723f99",
   "metadata": {},
   "source": [
    "## Loading API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c32b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "if not DEEPSEEK_API_KEY:\n",
    "    raise EnvironmentError('Set DEEPSEEK_API_KEY in your environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9628b78",
   "metadata": {},
   "source": [
    "### Initialising chat model and memory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41314ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_FILE = 'memory_store.json'\n",
    "CHAT_MODEL = 'deepseek-chat'\n",
    "TOP_K = 3 #change as required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d3b36",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a147d9",
   "metadata": {},
   "source": [
    "### Checking memory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbfbf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_memory_file():\n",
    "    if not os.path.exists(MEMORY_FILE):\n",
    "        with open(MEMORY_FILE, 'w') as f:\n",
    "            json.dump([], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f158ea",
   "metadata": {},
   "source": [
    "### Loading memory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7136b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memory() -> List[Dict[str, Any]]:\n",
    "    ensure_memory_file()\n",
    "    with open(MEMORY_FILE, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bcad1",
   "metadata": {},
   "source": [
    "### Saving the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5637fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_memory(mem: List[Dict[str, Any]]):\n",
    "    with open(MEMORY_FILE, 'w') as f:\n",
    "        json.dump(mem, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f88a52",
   "metadata": {},
   "source": [
    "### Embedding the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a7d5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text: str) -> List[float]:\n",
    "    np.random.seed(abs(hash(text)) % (2**32))\n",
    "    return np.random.rand(512).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc18e7",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473e28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    a = np.array(a, dtype=np.float32)\n",
    "    b = np.array(b, dtype=np.float32)\n",
    "    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22493834",
   "metadata": {},
   "source": [
    "### Taking top-k similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489380fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar(query: str, top_k: int = TOP_K) -> List[Dict[str, Any]]:\n",
    "    mem = load_memory()\n",
    "    if not mem:\n",
    "        return []\n",
    "    q_emb = embed_text(query)\n",
    "    scored = []\n",
    "    for entry in mem:\n",
    "        score = cosine_similarity(q_emb, entry['embedding'])\n",
    "        scored.append((score, entry))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [e for s,e in scored[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef1e9e",
   "metadata": {},
   "source": [
    "### Adding to the memory list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5033da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_memory(text: str, embedding: List[float]):\n",
    "    mem = load_memory()\n",
    "    entry = {\n",
    "        'id': str(uuid.uuid4()),\n",
    "        'text': text,\n",
    "        'embedding': embedding,\n",
    "        'ts': time.time()\n",
    "    }\n",
    "    mem.append(entry)\n",
    "    save_memory(mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8189c",
   "metadata": {},
   "source": [
    "### Making the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842b993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_input: str, retrieved: List[Dict[str, Any]]) -> str:\n",
    "    prompt = \"You are a helpful assistant. Use the relevant memories below when answering.\\n\\n\"\n",
    "    if retrieved:\n",
    "        prompt += \"Relevant memories:\\n\"\n",
    "        for i, r in enumerate(retrieved):\n",
    "            prompt += f\"[{i+1}] {r['text']}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    prompt += f\"User: {user_input}\\nAssistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd070162",
   "metadata": {},
   "source": [
    "### Calling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8996a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[{'role':'system','content':'You are a concise helpful agent.'},\n",
    "                  {'role':'user','content':prompt}],\n",
    "        max_tokens=300,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    reply = resp.choices[0].message.content.strip()\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3779829",
   "metadata": {},
   "source": [
    "### Main function for the interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive():\n",
    "    print('=== Minimal Memory-Augmented Chat Agent ===')\n",
    "    print('(type \"exit\" to quit, \"showmem\" to view memory)')\n",
    "    ensure_memory_file()\n",
    "    while True:\n",
    "        user_input = input('\\nYou: ').strip()\n",
    "        if user_input.lower() in ('exit','quit'):\n",
    "            break\n",
    "        if user_input.lower() == 'showmem':\n",
    "            mem = load_memory()\n",
    "            if not mem:\n",
    "                print('Memory is empty')\n",
    "            else:\n",
    "                for i,e in enumerate(mem):\n",
    "                    print(f\"{i+1}. {e['text']} (ts={int(e['ts'])})\")\n",
    "            continue\n",
    "\n",
    "        retrieved = retrieve_similar(user_input)\n",
    "        prompt = build_prompt(user_input, retrieved)\n",
    "        print('\\nAssistant: ...thinking...')\n",
    "        reply = call_llm(prompt)\n",
    "        print('\\rAssistant:', reply, \" \" * 10) \n",
    "        pair_text = f\"User: {user_input} | Assistant: {reply}\"\n",
    "        try:\n",
    "            emb = embed_text(pair_text)\n",
    "            add_memory(pair_text, emb)\n",
    "        except Exception as e:\n",
    "            print('Warning: failed to embed/save memory:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77d191",
   "metadata": {},
   "source": [
    "## Calling the \"interactive\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49018acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
